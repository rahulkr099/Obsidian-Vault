Alright Rahul â€” here comes a **high-quality, interview-grade backend machine coding round** for **Rate Limiting + Caching using Redis**.  
This is one of the most commonly asked backend challenges because it tests **performance, scalability, Redis usage, atomic operations, TTLs, and middleware design**.

Iâ€™ll keep it simple to read but deep in backend logic so you prepare like a pro.

---

# ğŸš¦ **Backend Machine Coding Round â€” Rate Limiting + Caching (Redis)**

### â± Time Limit: 60â€“80 minutes

### ğŸ¯ Goal

Build a backend system with:

- IP-based rate limiting (Redis-based)
    
- API response caching (Redis cache)
    
- Efficient TTL handling
    
- Clean middleware design
    
- Proper invalidation logic
    

---

# ğŸ“ **Problem Statement**

Design a Node.js backend with the following capabilities:

## **1. Redis-Based Rate Limiter**

- Allow **X requests per minute** per IP
    
- Use Redis atomic operations
    
- Return **429 Too Many Requests** when limit exceeded
    
- Track:
    
    - current request count
        
    - TTL of the limit window
        

## **2. Redis-Based Caching Layer**

- Cache results of a sample endpoint (e.g., `/products/:id`)
    
- Set TTL for cache (e.g., 60 seconds)
    
- Invalidate cache automatically on TTL expiry
    
- If data exists in Redis â†’ return cached response
    
- If not â†’ fetch from DB â†’ cache â†’ return
    

---

# ğŸ”§ **Required Endpoints**

```
GET  /products/:id
GET  /health
```

`/health` is a free endpoint (no rate limit, no caching).

---

# ğŸ“¦ **Functional Requirements**

## **1ï¸âƒ£ Rate Limiter Middleware (Redis)**

### Flow:

1. Extract IP
    
2. Redis key format:
    
    ```
    rate:<ip>
    ```
    
3. Use Redis atomic command:
    
    - `INCR`
        
    - `EXPIRE` (only if first request)
        
4. If count > limit â†’ return:
    
    ```json
    {
      "error": true,
      "message": "Too many requests"
    }
    ```
    
5. Otherwise â†’ allow request
    

### Expected Behavior:

- First request sets TTL
    
- All requests within the minute increment count
    
- After TTL â†’ counter resets automatically
    

---

## **2ï¸âƒ£ Caching Middleware (Redis)**

### Flow for `/products/:id`

1. Check Redis:
    
    - key: `cache:product:<id>`
        
2. If exists â†’ return cached JSON
    
3. Else:
    
    - Fetch from "fake DB"
        
    - Save to Redis with TTL (e.g., 60 seconds)
        
    - Return fresh data
        

### Fake Product Example:

```json
{
  "id": "p1",
  "name": "iPhone 15",
  "price": 78000
}
```

### Response:

```json
{
  "cached": false,
  "product": { ... }
}
```

Cache Version:

```json
{
  "cached": true,
  "product": { ... }
}
```

---

# ğŸ§ª **Edge Cases the Interviewer Will Test**

### Rate Limiting

- Redis down â†’ fallback to in-memory or allow
    
- IP missing / behind proxy â†’ read `X-Forwarded-For`
    
- Burst requests â†’ atomic Redis ops prevent race conditions
    
- Very small TTLs (<1 sec)
    
- Very high limits (e.g., 10,000/min)
    
- Abuse case:
    
    - Single user spams â†’ must correctly limit
        
- Multiple servers:
    
    - Redis ensures global rate limiting
        

### Caching

- Cache stampede (multiple requests before first DB fetch completes)
    
- Non-existent product â†’ decide:
    
    - Cache 404? or no-cache?
        
- Expired TTL â†’ fresh DB hit
    
- Cache inconsistency if DB updated
    
- Manual invalidation logic (optional)
    

---

# ğŸ§± **Backend Architecture Expected**

```
/middlewares
  - rateLimiterRedis.js
  - cacheRedis.js

/routes
  - products.js

/redis
  - redisClient.js
```

- Use redis client with:
    
    - `get`
        
    - `set`
        
    - `expire`
        
    - `incr`
        
- Must use **atomic INCR** for correctness
    

---

# âš™ï¸ **Performance Requirements**

- Rate limiter must be **O(1)** per request
    
- Cache lookup must be **O(1)**
    
- JSON data small (< 10KB)
    
- TTLs should be short to prevent stale data
    
- Redis should not block Node event loop
    

---

# ğŸ“Š **Expected Output Examples**

### Rate Limit Exceeded

```json
{
  "error": true,
  "message": "Too many requests, try again later"
}
```

### Cache Hit

```json
{
  "cached": true,
  "product": {
    "id": "p1",
    "name": "iPhone 15"
  }
}
```

---

# â­ **Interviewer Follow-Up Questions**

### Rate Limiting

1. Difference between:
    
    - Fixed window
        
    - Sliding window
        
    - Token bucket
        
    - Leaky bucket
        
2. Why Redis for distributed rate limiting?
    
3. How to rate limit:
    
    - per IP
        
    - per user
        
    - per API key
        
    - per route?
        
4. How to implement **global rate limiting**?
    
5. How to prevent slowloris attacks?
    

### Caching

1. When to invalidate cache?
    
2. When to NOT use caching?
    
3. What is a **cache stampede**? Solutions?
    
4. LRU cache vs TTL cache?
    
5. How to cache:
    
    - SQL results
        
    - complex objects
        
    - lists with pagination?
        

---

# ğŸŒŸ **Bonus Features to Impress**

- Add per-route custom rate limits
    
- Add hybrid memory + Redis caching
    
- Add cache warming
    
- Add `etag` header-based client caching
    
- Add Redis cluster + sharding
    
- Add request logs with hit/miss metrics
    
- Add queue-based background cache regeneration
    
- Add distributed lock (mutex) to avoid stampede
    

---

If you want, I can prepare the next backend challenge:

â¡ï¸ **Event Logging / Audit System**  
â¡ï¸ **Searchable Blog Engine**  
â¡ï¸ **CSV Import + Processing Pipeline**  
â¡ï¸ **Analytics Event Collector (like Segment)**

Just tell me the next round, Rahul!